# ğŸ™ï¸ AI Speech Emotion Recognition System

> Detect human emotions from speech audio using Machine Learning.

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Complete-brightgreen)]()

---

## ğŸ“Œ Problem Statement

Human communication is rich with emotional cues embedded in how words are spoken â€” not just what is said. Traditional text-based sentiment analysis misses prosodic features like pitch, tone, tempo, and energy that carry critical emotional information.

**Goal:** Build a system that can automatically recognise emotions (neutral, calm, happy, sad, angry, fearful, disgust, surprised) from short speech clips, enabling applications in:

- ğŸ¥ Mental health monitoring
- ğŸ“ Call-centre quality analysis
- ğŸ¤– Emotionally aware voice assistants
- ğŸ“ E-learning engagement tracking

---

## ğŸ’¡ Proposed Solution

An end-to-end pipeline that:

1. **Loads & preprocesses** raw `.wav` audio from the RAVDESS dataset.
2. **Extracts features** â€” MFCCs, Mel spectrograms, chroma, zero-crossing rate, RMS energy.
3. **Trains a Random Forest classifier** on the extracted features.
4. **Evaluates** with confusion matrices, classification reports, and per-emotion accuracy charts.
5. **Deploys** a simple Streamlit web app where users can upload audio and receive emotion predictions.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Audio   â”‚â”€â”€â”€â”€â–¶â”‚  Feature         â”‚â”€â”€â”€â”€â–¶â”‚  Random Forest â”‚
â”‚  (.wav)      â”‚     â”‚  Extraction      â”‚     â”‚  Classifier    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (MFCC, Mel,     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚   Chroma, ZCR)   â”‚             â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚  Prediction:  â”‚
                                               â”‚  "angry" ğŸ˜    â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“… 7-Day Development Roadmap

| Day | Focus Area | Key Deliverables | Status |
|-----|-----------|-----------------|--------|
| **1** | Project Setup & EDA | Folder structure, data loader, visualisations, README | âœ… Done |
| **2** | Feature Engineering | MFCC, chroma, Mel spectrogram extraction pipeline | âœ… Done |
| **3** | ML Model Training | Random Forest â€” train & evaluate | âœ… Done |
| **4** | Prediction Script | `predict.py` â€” predict emotion from any `.wav` file | âœ… Done |
| **5** | Evaluation Plots | Confusion matrix & per-emotion accuracy charts | âœ… Done |
| **6** | Web Interface | Streamlit app â€” upload audio â†’ get prediction | âœ… Done |
| **7** | Final Polish | Documentation, README update, final commit | âœ… Done |

---

## ğŸ“‚ Project Structure

```
Speech_emotion_recognition/
â”œâ”€â”€ data/                        # RAVDESS dataset (gitignored)
â”‚   â”œâ”€â”€ Actor_01/ â€¦ Actor_24/   # Raw .wav files
â”‚   â””â”€â”€ processed/               # Extracted features (.npy files)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.pkl                # Trained Random Forest model
â”‚   â””â”€â”€ scaler.pkl               # StandardScaler (used in prediction)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda.py                # Exploratory Data Analysis
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ confusion_matrix.png     # Confusion matrix heatmap
â”‚   â””â”€â”€ accuracy_per_emotion.png # Per-emotion accuracy bar chart
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py           # Dataset loading & parsing
â”‚   â”œâ”€â”€ extract_features.py      # Audio feature extraction
â”‚   â”œâ”€â”€ prepare_data.py          # Feature prep, scaling, train/test split
â”‚   â”œâ”€â”€ train_model.py           # Random Forest training
â”‚   â”œâ”€â”€ evaluate_model.py        # Evaluation charts
â”‚   â”œâ”€â”€ predict.py               # Predict emotion from a single file
â”‚   â””â”€â”€ visualize.py             # Waveform & spectrogram plotting
â”œâ”€â”€ app.py                       # Streamlit web application
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ—ƒï¸ Dataset â€” RAVDESS

**Ryerson Audio-Visual Database of Emotional Speech and Song**

| Property | Detail |
|----------|--------|
| Total files | 1 440 speech audio files |
| Actors | 24 (12 male, 12 female) |
| Emotions | 8 â€” neutral, calm, happy, sad, angry, fearful, disgust, surprised |
| Format | `.wav`, 16-bit, 48 kHz |
| Duration | ~3â€“5 seconds per clip |

### Filename Convention

```
{Modality}-{VocalChannel}-{Emotion}-{Intensity}-{Statement}-{Repetition}-{Actor}.wav
```

**Emotion codes:** `01`=neutral Â· `02`=calm Â· `03`=happy Â· `04`=sad Â· `05`=angry Â· `06`=fearful Â· `07`=disgust Â· `08`=surprised

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Language | Python 3.10+ |
| Audio processing | librosa, soundfile |
| Data handling | pandas, NumPy |
| Visualisation | matplotlib, seaborn |
| Machine Learning | scikit-learn (Random Forest) |
| Web app | Streamlit |
| Version control | Git + GitHub |

---

## ğŸš€ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/Nithin201342/Speech_emotion_recognition.git
cd Speech_emotion_recognition

# 2. Create & activate virtual environment
python -m venv .venv
.venv\Scripts\activate          # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Place the RAVDESS dataset
# Download from https://zenodo.org/record/1188976
# Extract into  data/  so the structure is  data/Actor_01/ â€¦ data/Actor_24/

# 5. Run the full pipeline
python src/prepare_data.py      # Extract features & save .npy files
python src/train_model.py       # Train the Random Forest model
python src/evaluate_model.py    # Generate evaluation charts

# 6. Predict emotion from a single audio file
python src/predict.py "data/Actor_01/03-01-01-01-01-01-01.wav"

# 7. Launch the web app
streamlit run app.py
```

---

## ğŸ“Š Results

- **Model:** Random Forest (100 trees)
- **Features:** MFCC (40) + Chroma (12) + Mel Spectrogram (128) + ZCR (1) + RMS (1) = **182 features**
- **Dataset:** 1440 audio files, 8 emotions, 24 actors
- **Train/Test split:** 80% / 20%
- **Accuracy:** ~72â€“78% on the held-out test set

---

## ğŸ“ License

This project is for educational and hackathon purposes.

---

*Built with â¤ï¸ for the MCA Hackathon â€” 7-Day Sprint*
