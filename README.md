<<<<<<< HEAD
# ğŸ™ï¸ AI Speech Emotion Recognition System

> Detect human emotions from speech audio using Machine Learning and Deep Learning.

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-In%20Progress-yellow)]()

---

## ğŸ“Œ Problem Statement

Human communication is rich with emotional cues that are often embedded in how words are spoken â€” not just what is said.  Traditional text-based sentiment analysis misses prosodic features like pitch, tone, tempo, and energy that carry critical emotional information.

**Goal:** Build a system that can automatically recognise emotions (neutral, calm, happy, sad, angry, fearful, disgust, surprised) from short speech clips, enabling applications in:

- ğŸ¥ Mental health monitoring
- ğŸ“ Call-centre quality analysis
- ğŸ¤– Emotionally aware voice assistants
- ğŸ“ E-learning engagement tracking

---

## ğŸ’¡ Proposed Solution

An end-to-end pipeline that:

1. **Loads & preprocesses** raw `.wav` audio from the RAVDESS dataset.
2. **Extracts features** â€” MFCCs, Mel spectrograms, chroma, zero-crossing rate, etc.
3. **Trains ML/DL models** â€” starting with classical classifiers (SVM, Random Forest) and progressing to deep learning (CNN / LSTM on spectrograms).
4. **Evaluates** with confusion matrices, classification reports, and per-emotion accuracy.
5. **Deploys** a simple web interface where users can upload or record audio and receive emotion predictions.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Audio   â”‚â”€â”€â”€â”€â–¶â”‚  Feature         â”‚â”€â”€â”€â”€â–¶â”‚  ML / DL       â”‚
â”‚  (.wav)      â”‚     â”‚  Extraction      â”‚     â”‚  Model         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (MFCC, Mel,     â”‚     â”‚  (SVM / CNN)   â”‚
                     â”‚   Chroma, ZCR)   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                                      â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚  Prediction:  â”‚
                                              â”‚  "angry" ğŸ˜    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“… 7-Day Development Roadmap

| Day | Focus Area | Key Deliverables |
|-----|-----------|-----------------|
| **1** âœ… | Project Setup & EDA | Folder structure, data loader, visualisations, README |
| **2** | Feature Engineering | MFCC, chroma, spectral contrast extraction pipeline |
| **3** | Classical ML Models | SVM, Random Forest, KNN â€” train & evaluate |
| **4** | Deep Learning | CNN on mel spectrograms, LSTM on MFCC sequences |
| **5** | Model Optimisation | Hyperparameter tuning, data augmentation, ensembles |
| **6** | Web Interface | Streamlit / Flask app â€” upload audio â†’ get prediction |
| **7** | Final Polish | Documentation, demo recording, testing, deployment prep |

---

## ğŸ“‚ Project Structure

```
Speech_emotion_recognition/
â”œâ”€â”€ data/                   # RAVDESS dataset (gitignored)
â”‚   â””â”€â”€ Actor_01/ â€¦ Actor_24/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda.py           # Exploratory Data Analysis
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py      # Dataset loading & parsing
â”‚   â””â”€â”€ visualize.py        # Waveform & spectrogram plotting
â”œâ”€â”€ models/                 # Saved trained models (gitignored)
â”œâ”€â”€ outputs/                # Plots & reports (gitignored)
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ—ƒï¸ Dataset â€” RAVDESS

**Ryerson Audio-Visual Database of Emotional Speech and Song**

| Property | Detail |
|----------|--------|
| Total files | 1 440 speech audio files |
| Actors | 24 (12 male, 12 female) |
| Emotions | 8 â€” neutral, calm, happy, sad, angry, fearful, disgust, surprised |
| Format | `.wav`, 16-bit, 48 kHz |
| Duration | ~3â€“5 seconds per clip |

### Filename Convention

Each filename contains 7 hyphen-separated numerical identifiers:

```
{Modality}-{VocalChannel}-{Emotion}-{Intensity}-{Statement}-{Repetition}-{Actor}.wav
```

**Emotion codes:** `01`=neutral Â· `02`=calm Â· `03`=happy Â· `04`=sad Â· `05`=angry Â· `06`=fearful Â· `07`=disgust Â· `08`=surprised

---

## ğŸ” Initial Observations (Day 1)

- [x] Dataset is well-structured with consistent filename conventions.
- [x] All 8 emotion categories are represented across 24 actors.
- [x] Audio clips are short (3â€“5 s) â€” suitable for fixed-length feature extraction.
- [ ] Class balance to be confirmed after full EDA.
- [ ] Signal-to-noise ratio quality to be assessed.
- [ ] Feature separability between similar emotions (e.g. calm vs neutral) to be explored.

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Language | Python 3.10+ |
| Audio processing | librosa, soundfile |
| Data handling | pandas, NumPy |
| Visualisation | matplotlib, seaborn |
| Classical ML | scikit-learn |
| Deep Learning | TensorFlow / Keras |
| Web app | Streamlit *(Day 6)* |
| Version control | Git + GitHub |

---

## ğŸš€ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/<your-username>/Speech_emotion_recognition.git
cd Speech_emotion_recognition

# 2. Create a virtual environment
python -m venv venv
source venv/bin/activate        # Linux / macOS
venv\Scripts\activate           # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Place the RAVDESS dataset
# Download from https://zenodo.org/record/1188976
# Extract into  data/  so the structure is  data/Actor_01/ â€¦ data/Actor_24/

# 5. Run the EDA notebook
jupyter notebook notebooks/01_eda.py
# or open in VS Code as an interactive Python file
```

---

## ğŸ“Š Day 1 Progress

- âœ… Professional project structure created
- âœ… README with problem statement, solution, and roadmap
- âœ… `data_loader.py` â€” loads RAVDESS dataset into a clean DataFrame
- âœ… `visualize.py` â€” waveform and mel spectrogram plotting functions
- âœ… EDA notebook with emotion distribution, duration analysis, and audio visualisations
- âœ… `requirements.txt` and `.gitignore`

---

## ğŸ“ License

This project is for educational and hackathon purposes.

---

*Built with â¤ï¸ for the MCA Hackathon â€” Day 1 of 7*
=======
# speech_emotion_recognition
>>>>>>> 697082aac9eada2d4dec7d661a6bb103f96fe46e
